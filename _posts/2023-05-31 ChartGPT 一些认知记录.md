---
title: ChartGPT 一些认识记录
author: nhsoft.lsd
date: 2023-05-31
categories: [AI]
tags: [AI,ChartGPT]
pin: true
---

# 几个基本概念

## ChatGPT
**ChatGPT**，全称聊天生成预训练转换器（英语：Chat Generative Pre-trained Transformer[2]），是OpenAI开发的人工智能聊天机器人程序，于2022年11月推出。该程序使用基于GPT-3.5、GPT-4架构的大型语言模型并以强化学习训练。

## 大型语言模型
**大型语言模型**，大型语言模型(LLM)是指包含数千亿（或更多）参数的语言模型，这些参数是在大量文本数据上训练的，例如模型GPT-3、PLM.
Galactica和LLaMA。具体来说，LLM建立在Transformer架构之上，其中多头注意力层堆叠在一个非常深的神经网络中。现有的LLM
主要采用与小语言模型类似的模型架构（即Transformer)和预训练目标（即语言建模）。

## NLP到大型语言模型的进阶历程

五个阶段：规则、统计机器学习、 深度学习、预训练、大型语言模型。

* **规则阶段**大致从1956年到1992年，基于规则的机器翻译系统是在内部把各种功能的模块串到一起，由人先从数据中获取知识，归纳出规则，写出来教给机器，然后机器来执行这套规则，从而完成
特定任务。
* **统计机器学习阶段**大致从1993年到2012年，机器翻译系统可拆成语言模型和翻译模型，这里的语言模型与现在的GPT-3/3.5的技术手段一模一样。该阶段相比上一阶段突变性较高，由人转述知
识变成机器自动从数据中学习知识，主流技术包括SVM、HMM、MaxEnt、CRF、LM等，当时人工标注数据量在百万级左右。
* **深度学习阶段**大致从2013-2018年，相对上一阶段突变性较低，从离散匹配发展到embedding:连续匹配，模型变得更大。该阶段典型技术栈包括Encoder-Decoder、.LSTM、Attention、.
Embedding等，标注数据量提升到千万级。
* **预训练阶段**是从2018年到2022年，相比之前的最大变化是加入自监督学习，张俊林认为这是NLP领域最杰出的贡献，将可利用数据从标注数据拓展到了非标注数据。该阶段系统可分为预训练和
微调两个阶段，将预训练数据量扩大3到5倍，典型技术栈包括Encoder--Decoder、.Transformer、.Attention等。
* **大型语言模型阶段**从2023年起，目的是让机器能听懂人的命令、遵循人的价值观。其特性是在第一个阶段把过去的两个阶段缩成一个预训练阶段，第二阶段转换成与人的价值观对齐，而不是向领
域迁移。这个阶段的突变性是很高的，已经从专用任务转向通用任务，或是以自然语言人机接口的方式呈现。

## 大型语言模型的涌现能力
LLM的三种代表性的涌现能力：
* **上下文学习**。GPT-3正式引入了上下文学习能力：假设语言模型已经提供了自然语言指令和多个任务描述，它可以通过完成输入文本的词序列来生成测试实例的预期输出，而无需额外的训练或
  梯度更新。
* **指令遵循**。通过对自然语言描述（即指令）格式化的多任务数据集的混合进行微调，LLM在微小的任务上表现良好，这些任务也以指令的形式所描述。这种能力下，指令调优使LLM能够在不使
  用显式样本的情况下通过理解任务指令来执行新任务，这可以大大提高泛化能力。
* **循序渐进的推理**。对于小语言模型，通常很难解决涉及多个推理步骤的复杂任务，例如数学学科单词问题。同时，通过思维链推理策略，LLM可以通过利用涉及中间推理步骤的pOt机制来解
  决此类任务得出最终答案。据推测，这种能力可能是通过代码训练获得的。

